# For splitting bams
def get_donors(wildcards, input):
	hash_file=pd.read_csv(input.barcodes_vs_donor, sep='\t')
	# Make this generalized with loc - bc hash co0lumn names in config.yaml file
	return ' '.join(hash_file.iloc[:, 0].unique().tolist())






rule create_inp_splitBams:
    input:
        f"{config['demux_pipeline']['final_count_matrix_dir']}{config['fold_struct_demux']}{config['demux_pipeline']['final_count_matrix_h5ad']}"

    priority: 7

    output:
        f"{config['split_bams_pipeline']['inp_split_bams_dir']}{config['fold_struct_bam_split1']}_bc_hash.txt"

    params:
        overwrite=config['split_bams_pipeline']['overwrite']


    threads: 1

    resources:
        mem_mb=allocate_mem_CIS,
        time_min=10

    shell:
        """
        if [ "{params.overwrite}" == True ]; then
            python3 helper_py_scripts/create_inp_splitBam.py {input} {output} --overwrite

        else
            python3 helper_py_scripts/create_inp_splitBam.py {input} {output}

        fi
        sleep 60
        """


rule filt_chr_bams:
    input:
        f"{config['STAR_solo_pipeline']['bams_dir']}{config['fold_struct']}{config['STAR_solo_pipeline']['bam']}"

    output:
        temp(f"{config['STAR_solo_pipeline']['bams_dir']}{config['fold_struct']}{config['split_bams_pipeline']['short_bam']}")

    params:
        temp_bam=f"{config['split_bams_pipeline']['sort_temp_dir']}temp_bam/{{num}}_{{id1}}.bam"


    threads: 1

    resources:
        mem_mb=allocate_mem_FCB,
        time_min=40

    shell:
        """
        ml samtools
        set -x
        samtools view {input} 1 -bho {output}
        """


rule split_bams:
    input:
        short_bam=f"{config['STAR_solo_pipeline']['bams_dir']}{config['fold_struct']}{config['split_bams_pipeline']['short_bam']}",
        barcodes_vs_donor=f"{config['split_bams_pipeline']['inp_split_bams_dir']}{config['fold_struct_bam_split1']}_bc_hash.txt" # with headers

    priority: 7

    params:
        # split_at=config['split_bams_pipeline']['bc_per_donor'], # Split barcodes if more than this number belonging to the same donor (can't merge files more than what specified by `ulimit -n`)
        # temp_dir=f"{config['split_bams_pipeline']['temp_dir']}",
        temp_bam_per_cell_dir=f"{config['split_bams_pipeline']['new_temp_dir']}{config['fold_struct_bam_split2']}",
        split_bams_dir=f"{config['split_bams_pipeline']['split_bams_dir2']}{config['fold_struct_bam_split2']}",
        n_donors=get_donors


    threads: 1

    resources:
        mem_mb=allocate_mem_SB,
        time_min=10

    output:
        f"{config['split_bams_pipeline']['split_bams_proxy_dir']}{config['fold_struct_bam_split1']}.txt"  # Proxy to the output

    run:
        hash_file=pd.read_csv(input.barcodes_vs_donor, sep='\t')
        for donor in hash_file.iloc[:, 0].unique():
            shell("""
                set -x
    			j="NPSAD_donor_$(uuidgen)" # jobID-per-donor
                if [ ! -f "{params.split_bams_dir}{i}.bam" ]
                then
            		bsub -J NPSAD-spl_bam_${{j}} -P acc_CommonMind -q express -n 1 -R span[hosts=1] -R rusage[mem=200] -W 00:10 -oo /sc/arion/projects/psychAD/pnm/logs_per_donor/new_split_bam_${{j}}.stdout -eo /sc/arion/projects/psychAD/pnm/logs_per_donor/new_split_bam_${{j}}.stderr -L /bin/bash "bash helper_sh_scripts/create_per_donor_bams.bash {i} {input.barcodes_vs_donor} {params.temp_bam_per_cell_dir} {params.split_bams_dir} {input.short_bam}" 
                else
                    echo "Skipped donor {i} for the bam file {input.short_bam}" >> {output}
                fi
                echo "Submitted script for donor {i}" >> {output}
                sleep 60
            """, i=donor)