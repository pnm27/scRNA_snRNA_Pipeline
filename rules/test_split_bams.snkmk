# For splitting bams
def get_donors(wildcards, input):
	hash_file=pd.read_csv(input.barcodes_vs_donor, sep='\t')
	# Make this generalized with loc - bc hash co0lumn names in config.yaml file
	return ' '.join(hash_file.iloc[:, 0].unique().tolist())






rule create_inp_splitBams:
    input:
        f"{config['demux_pipeline']['final_count_matrix_dir']}{config['fold_struct_demux']}{config['demux_pipeline']['final_count_matrix_h5ad']}"

    priority: 7

    output:
        f"{config['split_bams_pipeline']['inp_split_bams_dir']}{config['fold_struct_bam_split1']}_bc_hash.txt"

    params:
        overwrite=config['split_bams_pipeline']['overwrite']

    group: "split_bams"

    threads: 2

    resources:
        mem_mb=1000,
        time_min=10

    shell:
        """
        if [ "{params.overwrite}" == True ]; then
            python3 helper_py_scripts/create_inp_splitBam.py {input} {output} --overwrite

        else
            python3 helper_py_scripts/create_inp_splitBam.py {input} {output}

        fi
        sleep 60
        """


rule bamsort_by_CB:
    input:
        f"{config['STAR_solo_pipeline']['bams_dir']}{config['fold_struct']}{config['STAR_solo_pipeline']['bam']}",
        f"{config['split_bams_pipeline']['inp_split_bams_dir']}{config['fold_struct_bam_split1']}_bc_hash.txt"

    output:
        f"{config['STAR_solo_pipeline']['bams_dir']}{config['fold_struct']}{config['split_bams_pipeline']['filt_bam']}"

    params:
        temp_pref=f"{config['split_bams_pipeline']['sort_temp_dir']}temp/{{num}}_{{id1}}",
        temp_bc=f"{config['split_bams_pipeline']['sort_temp_dir']}temp_bc/{{num}}_{{id1}}_bc.txt"
        # temp_bam=f"{config['split_bams_pipeline']['sort_temp_dir']}temp_bam/{{num}}_{{id1}}.bam"

    group: "split_bams"

    threads: 4

    resources:
        mem_mb=5000,
        time_min=300

    shell:
        """
        mkdir -p {config[split_bams_pipeline][sort_temp_dir]}
        ml samtools
        cut -f2 <(tail -n +2 {input[1]}) > {params.temp_bc}
        samtools view -D CB:{params.temp_bc} {input[0]} -bho {output}
        rm {params.temp_bc}
        """


# rule split_bams:
#     input:
#         sort_bam=f"{config['STAR_solo_pipeline']['bams_dir']}{config['fold_struct']}{config['split_bams_pipeline']['sort_cb_bam']}",
#         barcodes_vs_donor=f"{config['split_bams_pipeline']['inp_split_bams_dir']}{config['fold_struct_bam_split1']}_bc_hash.txt" # with headers

#     priority: 7

#     params:
#         split_at=config['split_bams_pipeline']['bc_per_donor'], # Split barcodes if more than this number belonging to the same donor (can't merge files more than what specified by `ulimit -n`)
#         # temp_dir=f"{config['split_bams_pipeline']['temp_dir']}",
#         temp_bam_per_cell_dir=f"{config['split_bams_pipeline']['bams_per_barcode_dir']}{config['fold_struct_bam_split2']}",
#         split_bams_dir=f"{config['split_bams_pipeline']['split_bams_dir']}{config['fold_struct_bam_split2']}",
#         n_donors=get_donors

#     group: "split_bams"

#     threads: 1

#     resources:
#         mem_mb=30000

#     output:
#         f"{config['split_bams_pipeline']['split_bams_proxy_dir']}{config['fold_struct_bam_split1']}.txt"  # Proxy to the output

#     shell:
#         """
#         read -r -a donor <<< {params.n_donors}
#         for i in "${donor[@]}"
#         do
#         	{
# 				jobid="NPSAD_donor_$(uuidgen)" # per-donor
# 				bsub -J NPSAD-new_split_bam -P acc_CommonMind -q premium -n 1 -R span[hosts=1] -R rusage[mem=1000] \
# 				-W 48:00 -oo /sc/arion/projects/psychAD/pnm/new_split_bam.stdout -eo /sc/arion/projects/psychAD/pnm/new_split_bam.stderr -L /bin/bash "bash helper_sh_scripts/create_per_donor_bams ${i} \
# 				{input.barcodes_vs_donor}" 
#         	}
#         done
#         bash {input[0]} &> {output}
#         bash helper_sh_scripts/create_bams_per_donor.sh {input.barcodes_vs_donor} {params.split_bams_dir} {params.temp_bam_per_cell_dir} {params.split_at} >> {output} 2>&1
#         sleep 60
#         """


rule split_bams:
    input:
        short_bam=f"{config['STAR_solo_pipeline']['bams_dir']}{config['fold_struct']}{config['split_bams_pipeline']['filt_bam']}",
        barcodes_vs_donor=f"{config['split_bams_pipeline']['inp_split_bams_dir']}{config['fold_struct_bam_split1']}_bc_hash.txt" # with headers

    priority: 7

    params:
        # split_at=config['split_bams_pipeline']['bc_per_donor'], # Split barcodes if more than this number belonging to the same donor (can't merge files more than what specified by `ulimit -n`)
        # temp_dir=f"{config['split_bams_pipeline']['temp_dir']}",
        temp_bam_per_cell_dir=f"{config['split_bams_pipeline']['new_temp_dir']}{config['fold_struct_bam_split2']}",
        split_bams_dir=f"{config['split_bams_pipeline']['split_bams_dir']}{config['fold_struct_bam_split2']}",
        per_donor_log_dir=config['split_bams_pipeline']['per_donor_split_log_dir']
        # n_donors=get_donors


    threads: 1

    resources:
        mem_mb=allocate_mem_SB
        # time_min=10

    output:
        f"{config['split_bams_pipeline']['split_bams_proxy_dir']}{config['fold_struct_bam_split1']}.txt"  # Proxy to the output

    run:
        hash_file=pd.read_csv(input.barcodes_vs_donor, sep='\t')
        # bam_files=0
        proc_donors=[d for d in hash_file.iloc[:, 0].unique() if not os.path.isfile(os.path.join(params.split_bams_dir, f"{d}.bam"))]
        job_name_l=[]
        samp_num=('-'.join(os.path.basename(input.short_bam).replace('_chr1.bam', '').split('-')[1:3]))
        # If proc_donors is empty don't run
        if proc_donors:
            for donor in proc_donors:
                # bam_files+=1
                jname=f"NPSAD_{samp_num}_" + list(shell("uuidgen", iterable=True))[0]
                job_name_l.append(jname)
                shell("""
                    jid=$(bsub -J {j} -P acc_CommonMind -q express -n 1 -R span[hosts=1] -R rusage[mem=200] -W 00:20 -oo {params.per_donor_log_dir}{j}.stdout -eo {params.per_donor_log_dir}{j}.stderr -L /bin/bash "bash helper_sh_scripts/create_per_donor_bams.bash {i} {input.barcodes_vs_donor} {params.temp_bam_per_cell_dir} {params.split_bams_dir} {input.short_bam}")
                    jid=$(echo $jid | head -n1 | cut -d '<' -f2 | cut -d '>' -f1)
                    echo "Submitted script for donor {i} with jobid: ${{jid}}" >> {output}
                    sleep 10
                """, i=donor, j=jname)

            with open(output[0], "a") as fout:
                fout.write(f"Number of 'new' bam files expected at the completion of all the scripts for the bam file {input.short_bam} is {len(job_name_l)}")

        else:
            with open(output[0], "a") as fout:
                fout.write(f"Skipped bam file {input.short_bam} as all {len(hash_file.iloc[:, 0].unique())} donor file(s) was(were) already present in the given output_folder {params.split_bams_dir}")